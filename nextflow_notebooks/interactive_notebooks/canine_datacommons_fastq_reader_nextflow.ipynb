{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c9f5ca-6244-470d-bddb-681f70d7e229",
   "metadata": {},
   "source": [
    "*This tutorial provides a summarized explanation of Nextflow, derived from the original [Nextflow training documents](https://training.nextflow.io/). In addition, it also showcases an interactive example of how to utilize Nextflow in BRH adapted from the [Canine Data Commons FASTQ Reader tutorial](https://brh.data-commons.org/dashboard/Public/notebooks/canine_datacommons_fastq_reader.html).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdca20-1369-4b62-857f-4ab0b19fae18",
   "metadata": {},
   "source": [
    "In order for this tutorial to be able to download data from Gen3, access to the Tutorial CANINE Google Login must be authorized under the Profile page in BRH. Pulling the data from gen3 may not work on your local machine. [Read more](https://brh.data-commons.org/dashboard/Public/index.html#LinkingAccessTo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1378c-5bbf-4b67-97ca-79e092521694",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Nextflow Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dedd8dd-ebd8-4d0e-aac9-6072ce2f4f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Why Use Nextflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884692a-9af0-49f7-ac92-f8efbfe761df",
   "metadata": {},
   "source": [
    "Nextflow is a tool that helps to easily create workflows for data-heavy computations. It's built around the idea that Linux, with its many command-line and scripting tools, can be used to create data science pipelines.\n",
    "\n",
    "Nextflow allows you to define complex interactions between programming languages and gives you a sophisticated parallel computing environment in which to write pipelines (termed workflows). It's key features include:\n",
    "* Workflow portability and reproducibility\n",
    "* Scalability of parallelization and deployment\n",
    "* Integration of existing tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90939c02-3405-4779-8c9b-5aff028038c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Processes and Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3bf59-80e1-4e16-9ac7-7b2bab3d2217",
   "metadata": {},
   "source": [
    "A Nextflow workflow is created by connecting different processes. Each process can be written in any scripting language that Linux can run (like Bash, Perl, Ruby, Python, etc.). These processes run independently and do not share any common state that can be written to.\n",
    "\n",
    "The only way these processes can talk to each other is through asynchronous queues called channels. Any process can define one or more channels as an input and output. The way these processes interact, and therefore the workflow itself, is determined by these input and output declarations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ad1a6-46d6-4b7c-9757-c3de0a7cd0d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Execution Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2afedc7-a964-4f77-accb-ef514b8dae47",
   "metadata": {},
   "source": [
    "While a process defines what command or script should be run, the executor determines how that script is run on the target platform. Unless specified otherwise, processes are run on your local computer.\n",
    "\n",
    "However, for real-world workflows, a cloud platform is often needed. Nextflow provides a separation between the logic of the workflow and the system on which it runs. This means you can create a workflow that runs on your computer, a server, or the cloud, without needing any changes. You just need to define the target platform in the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0534fb-475f-4be0-8531-b297ace0af5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scripting Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa134dd6-7900-42c1-9dfd-b2f483a2d00c",
   "metadata": {},
   "source": [
    "Nextflow scripting is an extension of the Groovy programming language, which itself is a subset of Java. Groovy is like a simplified version of Java, making it easier and more approachable to write code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc302c3-b375-4e43-b464-e664d16b732f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example Gen3 Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4729a651-4b47-40d5-928c-104aa30c5708",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60214f6a-3233-4e92-9a01-6058fcb4b178",
   "metadata": {},
   "source": [
    "Now let's get into the specifics of how Nextflow works. The most essential component of a Nextflow workflow is the main .nf Nextflow script file. Here is where the main flow of the workflow is set. We will slowly build up the components of this file in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9219c1-3714-446c-adf4-e8d6bdac1968",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/usr/bin/env nextflow\n",
    "...\n",
    "workflow {\n",
    "    files = DownloadFastqFile()\n",
    "    files.flatten().set {input_files}\n",
    "    AnalyzeFastqFile(input_files)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa29550-0ff5-48a8-ae8a-d3a345c75ebe",
   "metadata": {},
   "source": [
    "The first line of a Nextflow script file is a shebang. It's a special type of comment used in scripts to specify what interpreter should be used to execute the script.\n",
    "\n",
    "In this case, `#!/usr/bin/env nextflow` tells the system to execute the script using the `nextflow` interpreter. The `env` command helps locate the `nextflow` interpreter within the system's PATH. If you are using a Nextflow workspace in BRH, this setup has already been properly done. If not, refer to https://www.nextflow.io/docs/latest/getstarted.html for setup instructions.\n",
    "\n",
    "The workflow is the final component of the Nextflow script and outlines the primary steps in the workflow:\n",
    "\n",
    "1. `files = DownloadFastqFile()`: This line executes the `DownloadFastqFile` process, which is defined earlier in the script (not shown here for simplicity). The results of this process, which could be multiple files, are stored in the variable `files`.\n",
    "\n",
    "2. `files.flatten().set {input_files}`: The `flatten()` operator transforms the nested collection of files into a flat list. This step is necessary if `DownloadFastqFile` outputs a list of lists of files. This flattened list of files is then converted into a Nextflow channel and stored in `input_files`. A Nextflow channel is a mechanism that allows data to be passed between processes.\n",
    "\n",
    "3. `AnalyzeFastqFile(input_files)`: This line calls the `AnalyzeFastqFile` process, using the output of `DownloadFastqFile` (the flattened list of files) as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7b26a-4a1f-49fd-adde-a1c740aabd73",
   "metadata": {},
   "source": [
    "The workflow showcased here consists of two separate processes. The first of these is `DownloadFastqFile`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc7f2a-2d9d-45ea-9d69-96e79e89becb",
   "metadata": {},
   "source": [
    "```bash\n",
    "process DownloadFastqFile {\n",
    "\n",
    "    // Move the outputs of DownloadFastqFile to the results directory in\n",
    "    // the base workflow directory\n",
    "\n",
    "    conda './environment.yml' // path to your environment.yml file\n",
    "\n",
    "    output:\n",
    "    path(\"*.fastq\")\n",
    "\n",
    "    script:\n",
    "    \"\"\"\n",
    "    gen3 drs-pull object \"${params.fastq_guid}\"\n",
    "    for file in \\$(ls | grep .fastq.gz); do\n",
    "        gunzip \"\\$file\"\n",
    "    done\n",
    "    \"\"\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd3cf6-592e-4f67-8125-a0c8308a9746",
   "metadata": {},
   "source": [
    "This process consists of several major components:\n",
    "\n",
    "1. `conda './environment.yml'`: This is an optional line which tells Nextflow to use the conda environment specified in the 'environment.yml' file to run this process (or create it if it doesn't already exist). Conda is a package and environment management system, and using this statement ensures that the correct dependencies are installed and used for this process. It is also possible to specify the conda environment to be used in the `nextflow.config` file instead (see below for more details on the Nextflow configuration file).\n",
    "\n",
    "2. `output: path(\"*.fastq\")`: This line specifies what the process will output. In this case, the process outputs one or more fastq files pulled from gen3.\n",
    "\n",
    "3. `script:`: This begins the section where the commands to be run by this process are defined. In this case the commands are a bash script to download the `params.fastq_guid` (which is a parameter to the nextflow workflow which is given in the configuration file - more details below) file from gen3 and a loop to go through all the files ending in .fastq.gz that were pulled and unzip them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90093015-71e2-4704-bd36-e10fe85f8075",
   "metadata": {},
   "source": [
    "The `./environment.yml` file being used to specify the conda environment installs the needed dependencies for the python scripts being run. Read more about conda environment files [here](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345a7d88-3486-4da1-aaad-74d9a4936f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing environment.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile environment.yml\n",
    "name: canine\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip\n",
    "  - pip:\n",
    "    - gen3\n",
    "    - bioinfokit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59941ea-c86e-4fe0-9a6b-e57e30ddbc15",
   "metadata": {},
   "source": [
    "*Note the `%%writefile environment.yml` line tells Jupyter Notebook to write the cell to a file rather than running the code in Jupyter Notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06016bb7-5398-494b-bfd0-eefd48ee1b55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing nextflow.config\n"
     ]
    }
   ],
   "source": [
    "%%writefile nextflow.config\n",
    "// enables the processes to be run in a conda environment\n",
    "conda.enabled = true\n",
    "\n",
    "params {\n",
    "    fastq_guid = \"dg.C78ne/4527012c-3a5f-481d-820c-da7b77a26b48\" // GUID for fastq file\n",
    "    max_records = 10 // max records to process\n",
    "    endpoint = \"https://caninedc.org\"\n",
    "    refresh_file = \"/home/jovyan/.gen3/credentials.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3103353",
   "metadata": {},
   "source": [
    "The Nextflow configuration file gives additional setup details for the Nextflow workflow. In this case, it specifies that Conda is to be used for setting up the environments for the individuals processes and sets up some default parameters for the Nextflow script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd6128",
   "metadata": {},
   "source": [
    "These parameters can be used within the script and can also be overridden when you run the script. \n",
    "\n",
    "You can set or override the parameters when you run the script by passing them as command line arguments with the `--` prefix. \n",
    "\n",
    "For example, to run the script with different values for `refresh_file` and `max_records`, you would use the following command:\n",
    "\n",
    "```bash\n",
    "nextflow run canine.nf --refresh_file=\"/home/josh/.gen3/credentials.json\" --max_records=20\n",
    "```\n",
    "\n",
    "`nextflow run canine.nf` is the ordinary syntax for running the script, `/home/josh/.gen3/credentials.json` is the new value for `refresh_file` and `20` is the new value for `max_records`. The `max_records` parameter is accessed in the Nextflow script itself through `{params.max_records}`.\n",
    "\n",
    "This is one of the features that makes Nextflow powerful for pipeline scripting. By using parameters, the same script can be used with different data or settings without any changes to the script itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b5830-0fce-4f34-b6fe-70b962cd1cc9",
   "metadata": {},
   "source": [
    "Nextflow configuration files can specify variables for the script being run or specific settings for how to run the script. Read more about Nextflow configuration files at https://www.nextflow.io/docs/latest/config.html ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef10ae-43a4-4383-b1e9-076f061fcd6a",
   "metadata": {},
   "source": [
    "```bash\n",
    "process AnalyzeFastqFile {\n",
    "\n",
    "    // Copy the outputs of AnalyzeFastqFile to the results directory in\n",
    "    // the base workflow directory\n",
    "    publishDir \"${baseDir}/results\", mode: 'copy'\n",
    "    \n",
    "    conda './environment.yml' // path to your environment.yml file\n",
    "\n",
    "    input:\n",
    "    path(input_file)\n",
    "\n",
    "    output:\n",
    "    path(\"${input_file}_analysis.txt\")\n",
    "\n",
    "    script:\n",
    "    \"\"\"\n",
    "    python3 ${baseDir}/analyze_fastq.py ${params.endpoint} ${params.refresh_file} ${input_file} ${params.max_records}\n",
    "    \"\"\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33650e1c-6981-4296-a313-33f8736445cc",
   "metadata": {},
   "source": [
    "The second process in this workflow takes the file specified by the parameter `params.fastq_file` and runs the Python script `analyze_fastq.py` which can be found in the same directory as the Nextflow script (this is indicated by the `${baseDir}/` before the name of the Python script to be run).\n",
    "\n",
    "The `analyze_fastq.py` script is run with four command-line parameters each of which is specified by one of the parameters specified in the configuration file. Nextflow automatically stores the path for the input file to the process inside the input file variable which can then be referenced in the output file name as well as given as a command line parameter to the python script.\n",
    "\n",
    "Lastly, the AnalyzeFastqFile process also specifies `${input_file}_analysis.txt` as the output of the `analyze_fastq.py`. The actual code to create the file is within the Python script itself but this section indicates to the process to expect a text file named `${input_file}_analysis.txt` to be written by the Python script. The line `publishDir \"${baseDir}/results\", mode: 'copy'` indicates for Nextflow to copy the `${params.fastq_file}_analysis.txt` file produced by this process to the results folder in the same main directory as the Nextflow script (`${baseDir}`). This line will also automatically create a results folder for the output to be copied into if one doesn't already exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef7bee7-bf44-4c3a-be65-a6e535b4cb85",
   "metadata": {},
   "source": [
    "The Python code for the AnalyzeFastqFile process is given in `analyze_fastq.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e51afc1-d7e8-49ac-be04-a0ece8140e52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing analyze_fastq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile analyze_fastq.py\n",
    "import argparse\n",
    "from gen3.file import Gen3File\n",
    "from gen3.query import Gen3Query\n",
    "from gen3.auth import Gen3Auth\n",
    "from gen3.submission import Gen3Submission\n",
    "from gen3.index import Gen3Index\n",
    "from bioinfokit.analys import fastq\n",
    "\n",
    "# create the top-level parser\n",
    "parser = argparse.ArgumentParser(prog='analyze_fastq')\n",
    "parser.add_argument('endpoint', type=str, help='The endpoint.')\n",
    "parser.add_argument('refresh_file', type=str, help='The refresh file.')\n",
    "parser.add_argument('fastq_file', type=str, help='The FASTQ file to analyze.')\n",
    "parser.add_argument('record_limit', type=float, help='The maximum number of records to process.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "endpoint = args.endpoint\n",
    "auth = Gen3Auth(endpoint, refresh_file=args.refresh_file)\n",
    "sub = Gen3Submission(endpoint, auth)\n",
    "file = Gen3File(endpoint, auth)\n",
    "\n",
    "programs = sub.get_programs()\n",
    "\n",
    "records = fastq.fastq_reader(file=args.fastq_file)\n",
    "counter = 0\n",
    "output_file = open(args.fastq_file + \"_analysis.txt\", \"w\")\n",
    "\n",
    "for record in records:\n",
    "    if counter < args.record_limit:\n",
    "        _, sequence, _, quality = record\n",
    "        base_count = {'A': sequence.count('A'), 'C': sequence.count('C'), 'G': sequence.count('G'), 'T': sequence.count('T')}\n",
    "        output_file.write(f\"{sequence}, {quality}, {base_count}\\n\")\n",
    "    else:\n",
    "        break\n",
    "    counter += 1\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76588f-b9b8-4f24-9447-9bbf69b4725c",
   "metadata": {},
   "source": [
    "This script, `analyze_fastq.py`, is used to analyze a FASTQ file and create a textual analysis output file. This analysis consists of the sequence, quality, and base counts for a specified number of records. The FASTQ file to be analyzed, the number of records to be analyzed, and a few more parameters are passed to the script as command-line arguments.\n",
    "\n",
    "Let's go through the script line by line:\n",
    "\n",
    "1. `import argparse`: This line imports the `argparse` module, which provides functions to facilitate the parsing of command-line arguments.\n",
    "\n",
    "2. The script then imports several modules from the `gen3` and `bioinfokit` packages. These packages are used to interact with a Gen3 data repository and analyze the FASTQ file respectively.\n",
    "\n",
    "3. The `argparse.ArgumentParser` function is used to create a parser object. This object will hold all the information necessary to parse the command-line arguments.\n",
    "\n",
    "4. `parser.add_argument` is then used to specify which command-line options the program is expecting. In this case, it is expecting four positional arguments: `endpoint`, `refresh_file`, `fastq_file`, and `record_limit`.\n",
    "\n",
    "5. `args = parser.parse_args()`: This line parses the command-line arguments and returns them as an `argparse.Namespace` object. The arguments are then accessed as attributes of this object.\n",
    "\n",
    "6. `auth = Gen3Auth(endpoint, refresh_file=args.refresh_file)`: This line creates a `Gen3Auth` object. This represents the user's authorization to access the Gen3 data repository. The `refresh_file` argument should point to a file containing the user's credentials.\n",
    "\n",
    "7. `sub = Gen3Submission(endpoint, auth)`: This line creates a `Gen3Submission` object which can be used to submit data to the Gen3 repository.\n",
    "\n",
    "8. `file = Gen3File(endpoint, auth)`: This line creates a `Gen3File` object, representing the file to be analyzed.\n",
    "\n",
    "9. `programs = sub.get_programs()`: This line gets a list of programs from the Gen3 repository.\n",
    "\n",
    "10. `records = fastq.fastq_reader(file=args.fastq_file)`: This line reads the records from the FASTQ file, which is one of the command-line arguments.\n",
    "\n",
    "11. An output file is opened for writing. The output file's name is the same as the input FASTQ file, with \"_analysis.txt\" appended to it.\n",
    "\n",
    "12. The script then enters a for loop, iterating over the FASTQ records. For each record, it splits the record into its sequence and quality components. It then counts the number of each base (A, C, G, T) in the sequence. This information is written to the output file. The loop stops when the number of processed records equals the `record_limit` argument.\n",
    "\n",
    "13. `output_file.close()`: Finally, the script closes the output file.\n",
    "\n",
    "In summary, this script (and by extension, the `AnalyzeFastqFile` process which calls it) reads a FASTQ file, analyzes the sequence and quality of a certain number of records, and writes the results to a new file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08daf31f-ba0f-49e3-9328-4c9f486d06d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Putting all sections of the Nextflow script together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1afb713d-83b3-40ef-baa6-553753969f53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing canine.nf\n"
     ]
    }
   ],
   "source": [
    "%%writefile canine.nf\n",
    "#!/usr/bin/env nextflow\n",
    "\n",
    "process DownloadFastqFile {\n",
    "\n",
    "    // Move the outputs of DownloadFastqFile to the results directory in\n",
    "    // the base workflow directory\n",
    "\n",
    "    conda './environment.yml' // path to your environment.yml file\n",
    "\n",
    "    output:\n",
    "    path(\"*.fastq\")\n",
    "\n",
    "    script:\n",
    "    \"\"\"\n",
    "    gen3 drs-pull object \"${params.fastq_guid}\"\n",
    "    for file in \\$(ls | grep .fastq.gz); do\n",
    "        gunzip \"\\$file\"\n",
    "    done\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "process AnalyzeFastqFile {\n",
    "\n",
    "    // Copy the outputs of AnalyzeFastqFile to the results directory in\n",
    "    // the base workflow directory\n",
    "    publishDir \"${baseDir}/results\", mode: 'copy'\n",
    "    \n",
    "    conda './environment.yml' // path to your environment.yml file\n",
    "\n",
    "    input:\n",
    "    path(input_file)\n",
    "\n",
    "    output:\n",
    "    path(\"${input_file}_analysis.txt\")\n",
    "\n",
    "    script:\n",
    "    \"\"\"\n",
    "    python3 ${baseDir}/analyze_fastq.py ${params.endpoint} ${params.refresh_file} ${input_file} ${params.max_records}\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "workflow {\n",
    "    files = DownloadFastqFile()\n",
    "    files.flatten().set {input_files}\n",
    "    AnalyzeFastqFile(input_files)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c19fd-6753-4ffc-9565-9c06fc4baeeb",
   "metadata": {},
   "source": [
    "To edit and re-run the Nextflow script, make sure to execute the above cell again to rewrite the Nextflow script file and then run the code block following this to re-run the script itself. It may also be necessary to delete the work and results directory between runs since the large size of the downloaded `SRR7012463_1.fastq` file can quickly exhaust memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea93d81-1156-444f-b064-f15c3a372be5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65f763a-ade7-4200-8282-bcecdde11441",
   "metadata": {},
   "source": [
    "Now all that's left is to run the Nextflow process itself (the resume option is optional and tells Nextflow to use any work files leftover from previous runs when able):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e2bd2e-ab72-4d2c-8615-f4ff4de5c004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 22.10.6\n",
      "Launching `canine.nf` [voluminous_spence] DSL2 - revision: 660eb25521\n",
      "[-        ] process > DownloadFastqFile -\u001b[K\n",
      "\u001b[2A\n",
      "[-        ] process > DownloadFastqFile [  0%] 0 of 1\u001b[K\n",
      "[-        ] process > AnalyzeFastqFile  -\u001b[K\n",
      "\u001b[3A\n",
      "executor >  local (1)\u001b[K\n",
      "[4e/d145d9] process > DownloadFastqFile [  0%] 0 of 1\u001b[K\n",
      "[-        ] process > AnalyzeFastqFile  -\u001b[K\n",
      "\u001b[4A\n",
      "executor >  local (1)\u001b[K\n",
      "[4e/d145d9] process > DownloadFastqFile [100%] 1 of 1 笨能u001b[K\n",
      "[-        ] process > AnalyzeFastqFile  -\u001b[K\n",
      "\u001b[4A\n",
      "executor >  local (2)\u001b[K\n",
      "[4e/d145d9] process > DownloadFastqFile    [100%] 1 of 1 笨能u001b[K\n",
      "[06/2b42a8] process > AnalyzeFastqFile (1) [  0%] 0 of 1\u001b[K\n",
      "\u001b[4A\n",
      "executor >  local (2)\u001b[K\n",
      "[4e/d145d9] process > DownloadFastqFile    [100%] 1 of 1 笨能u001b[K\n",
      "[06/2b42a8] process > AnalyzeFastqFile (1) [100%] 1 of 1 笨能u001b[K\n",
      "\u001b[32;1mCompleted at: 25-Jul-2023 19:43:57\n",
      "Duration    : 1m 13s\n",
      "CPU hours   : (a few seconds)\n",
      "Succeeded   : 2\n",
      "\u001b[22;39m\u001b[K\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nextflow run canine.nf --resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87e10e-a6c9-4794-94a3-9f090a80e7f6",
   "metadata": {},
   "source": [
    "Nextflow lists the process or processes it is running at any point when it is running via command-line output (displayed above). The hash corresponding to each process is displayed to the left of the process itself. At the end of its execution, Nextflow also returns the time-stamp of the workflow execution, the workflow real-time execution time, the time the workflow took on the devices used for execution, and the number of processes which ran successfullly at the end of its command-line output.\n",
    "\n",
    "When the script is run, Nextflow creates a directory structure to manage the data and workflow execution. Here is a breakdown of the resulting directory structure:\n",
    "\n",
    "1. `work/`: This directory is automatically created by Nextflow and is where the program runs the processes. Inside the `work/` directory, there will be multiple subdirectories, each corresponding to a process task. Each subdirectory will be named with a hash value, unique to each task (and its corresponding process). Inside these task directories, Nextflow stores scripts, input files, and output files related to each task. \n",
    "\n",
    "2. `results/`: This directory is specified in the `AnalyzeFastqFile` process with the `publishDir` directive. This is where the output files of the `AnalyzeFastqFile` process will be copied to.\n",
    "\n",
    "    - `results/SRR7012463_1.fastq_analysis.txt`: This file is the output of the `AnalyzeFastqFile` process. It contains the analysis of the FASTQ file and is copied into the `results/` directory.\n",
    "\n",
    "The file `SRR7012463_1.fastq` downloaded from the `DownloadFastqFile` process is not specified to be copied or moved to a specific location, so it will also remain within its `work/` subdirectory. This file is not copied from its work directories since it is extremely large and is likely to exhaust memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
