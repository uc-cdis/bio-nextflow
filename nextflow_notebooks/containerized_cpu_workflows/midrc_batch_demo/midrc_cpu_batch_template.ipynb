{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168c4a22-3daa-45ac-affb-a587f8e3f28a",
   "metadata": {},
   "source": [
    "# MIDRC Nextflow CPU Batch Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ec2af-35dc-4b25-8aa2-19b3be3f4a35",
   "metadata": {},
   "source": [
    "### Define Workflow\n",
    "\n",
    "Define the processes of the workflow. In this workflow we will execute two processes in batch. The first process converts the dicom files to png files using a containerized script. The second process extracts metadata from the dicom files and writes the metadata to a csv file using a second containerized script. Before running this workflow, you need to run the 'midrc_download_dcm_conda.ipynb' workflow to download dicom files to your local workspace environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb0288-2eda-4092-9e53-59aca8c46adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.nf\n",
    "#!/usr/bin/env nextflow\n",
    "\n",
    "/* pipeline input parameters, update this to your data dir */\n",
    "dicom_data = \"$baseDir/sdk_data/53/dabffdcc40fbffe39d78e7a926e655/*.dcm\" # path to your downloaded dicom files.\n",
    "project_dir = projectDir\n",
    "\n",
    "process dicom_to_png {\n",
    "    \n",
    "    label 'dcm2png'\n",
    "    \n",
    "    input:\n",
    "    path dicom_files\n",
    "    \n",
    "    output:\n",
    "    stdout emit: dicom_to_png_log\n",
    "    path('*.png'), emit: png_files\n",
    "    \n",
    "    script:\n",
    "    \"\"\"\n",
    "    python3 /utils/dicom_to_png.py $dicom_files\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "process extract_metadata {\n",
    "    \n",
    "    label 'ext_metadata'\n",
    "    \n",
    "    input:\n",
    "    path dicom_files\n",
    "    \n",
    "    output:\n",
    "    stdout emit: extract_metadata_log\n",
    "    path('*.csv'), emit: csv_files\n",
    "    \n",
    "    script:\n",
    "    \"\"\"\n",
    "    python3 /utils/extract_metadata.py $dicom_files\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "// Define the entry workflow (initial workflow for Nextflow to run)\n",
    "workflow {\n",
    "   \n",
    "    dicom_files = Channel.fromPath(dicom_data)\n",
    "    dicom_to_png(dicom_files)\n",
    "    extract_metadata(dicom_files)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae69116-a503-46e9-bd8e-652abffdec71",
   "metadata": {},
   "source": [
    "### Define Workflow Containers And Resources\n",
    "\n",
    "Define the containers and compute resources used in the workflow. Each process in the workflow needs it's own defined container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca4c178-d739-4c04-850a-c756ff5429a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile nextflow.config\n",
    "\n",
    "process {\n",
    "    withLabel: dcm2png {\n",
    "        executor = 'awsbatch'\n",
    "        queue = 'placeholder'\n",
    "        container = 'placeholder'\n",
    "    } \n",
    "}\n",
    "\n",
    "process {\n",
    "    withLabel: ext_metadata {\n",
    "        executor = 'awsbatch'\n",
    "        queue = 'placeholder'\n",
    "        container = 'placeholder'\n",
    "    } \n",
    "}\n",
    "\n",
    "aws {\n",
    "    region = 'us-east-1'\n",
    "    batch {\n",
    "        cliPath = '/home/ec2-user/miniconda/bin/aws'\n",
    "        jobRole = 'placeholder'\n",
    "    }\n",
    "}\n",
    "workDir = 'placeholder'\n",
    "\n",
    "\n",
    "docker.enabled = true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4242ac-ac3a-40c9-8be8-3a0f79ba07e3",
   "metadata": {},
   "source": [
    "### Run Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11295b2-2cf7-4b6d-a280-1e40b2bc3c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nextflow run main.nf -dsl2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef4ca5-073e-417b-84fd-ada45828a575",
   "metadata": {},
   "source": [
    "## Gather Results\n",
    "- Gather the converted .png files\n",
    "- Pull down the metadata files for each dicom file and merge the metadata to a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565953b-b6b7-47ba-8e13-08e7f8e002b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b12a46-5a77-4978-93b4-07ad85d98e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a824a3f-73fe-4f3e-94b2-7d233a12b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the aws endpoints for each batch process. Since we are running two processes on 5 threads there will be 10 total endpoints.\n",
    "end_points = []\n",
    "with open(\".nextflow.log\", 'r') as f:\n",
    "    for line in f:\n",
    "        if \"COMPLETED\" in line:\n",
    "            end_points.append(line.split(' ')[-1][:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fac66-e0ad-4d36-a4c2-8be3f7679a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the results from each batch session. The results are all placed into a local 'results' folder.\n",
    "for i in range(len(end_points)):\n",
    "    command = f'aws s3 cp {end_points[i]}/ ./results/ --recursive --exclude \"*\" --include \"*\" --quiet'\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b6ec2-b1b2-47e0-9bfb-bdc38c299997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine inference results from each batch session and write the combined metadata to a local csv file.\n",
    "files = os.listdir('results/')\n",
    "results_df = pd.DataFrame()\n",
    "for file in files:\n",
    "    if file[-3:] == 'csv':\n",
    "        label = file.split('_')[-1].split('.')[0]\n",
    "        temp_df = pd.read_csv('results/' + file)\n",
    "        temp_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "        temp_df['Label'] = label\n",
    "        results_df = pd.concat([results_df, temp_df])\n",
    "\n",
    "results_df.to_csv('midrc_batch_dicom_metadata.csv', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0af642-120d-4f80-a711-0c6b732cbf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
